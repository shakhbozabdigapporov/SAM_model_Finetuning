{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a899a9-aebe-4542-8e17-57b34afe1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"wanglab/medsam-vit-base\")\n",
    "model = SamModel.from_pretrained(\"wanglab/medsam-vit-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d759df5c-e944-43e1-9198-19af2b1ef4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17b53294-a1f8-4a5b-882c-73e9e0e958b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 130\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3dfd180b-4746-4f94-9525-7a79f8d3b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Image(mode=None, decode=True, id=None), 'label': Image(mode=None, decode=True, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d0063-2498-454d-b324-c143789aa1d9",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5acae014-c264-404c-92a4-df0703ce975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Assuming image files are PNG and mask files are BMP\n",
    "        self.img_names = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        \n",
    "        # Construct paths to the image and mask\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_name = img_name.replace('.png', '.bmp')  # Replace .png with .bmp to get corresponding mask name\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Convert image to RGB\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Load mask (no need to convert, assume single channel)\n",
    "         \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d279a-2881-4ac3-8d16-c457db4d60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "img_dir = \"images folder\"\n",
    "mask_dir = \"masks folder\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  \n",
    "    transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "dataset = CustomSegmentationDataset(img_dir=img_dir, mask_dir=mask_dir, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "for images, masks in dataloader:\n",
    "    print(masks.shape)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cc25db99-cce2-40cb-bb86-3a14ff1863d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94795a-e384-4c07-9eb3-be0920a4eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, len(dataset) - 1)\n",
    "image, mask = dataset[random_idx]\n",
    "\n",
    "image_np = image.permute(1, 2, 0).numpy()  \n",
    "\n",
    "mask_np = mask.squeeze().numpy()  \n",
    "\n",
    "if mask_np.dtype != np.uint8:\n",
    "    mask_np = (mask_np * 255).astype(np.uint8)  \n",
    "\n",
    "mask_np = mask_np / mask_np.max()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_np)\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask_np, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.axis('off')\n",
    "print(mask_np.shape)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ce84f-6be9-43e9-9b1b-e89e506977ca",
   "metadata": {},
   "source": [
    "# Other way of loading the images by the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2d35c17-604a-4547-b2fe-22d6b84d4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape before filtering: (763, 512, 512, 3)\n",
      "Masks shape before filtering: (763, 512, 512)\n",
      "Images shape after filtering: (654, 512, 512, 3)\n",
      "Masks shape after filtering: (654, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "\n",
    "        self.image_names = os.listdir(images_dir)\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "\n",
    "        for image_name in self.image_names:\n",
    "            img_path = os.path.join(self.images_dir, image_name)\n",
    "            mask_name = image_name.replace(\".png\", \".bmp\")\n",
    "            mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "                image_np = np.array(image)\n",
    "                mask_np = np.array(mask)\n",
    "\n",
    "                self.images.append(image_np)\n",
    "                self.masks.append(mask_np)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image or mask: {e}\")\n",
    "\n",
    "        self.images = np.array(self.images)\n",
    "        self.masks = np.array(self.masks)\n",
    "\n",
    "        print(f\"Images shape before filtering: {self.images.shape}\")\n",
    "        print(f\"Masks shape before filtering: {self.masks.shape}\")\n",
    "\n",
    "        self._filter_empty_masks()\n",
    "\n",
    "    def _filter_empty_masks(self):\n",
    "        if self.masks.size == 0:\n",
    "            print(\"No masks loaded. Check data loading process.\")\n",
    "            self.images = np.array([])\n",
    "            self.masks = np.array([])\n",
    "            return\n",
    "        valid_indices = [i for i, mask in enumerate(self.masks) if mask.max() != 0]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(\"No non-empty masks found.\")\n",
    "            self.images = np.array([])\n",
    "            self.masks = np.array([])\n",
    "            return\n",
    "\n",
    "        self.images = self.images[valid_indices]\n",
    "        self.masks = self.masks[valid_indices]\n",
    "\n",
    "        print(f\"Images shape after filtering: {self.images.shape}\")\n",
    "        print(f\"Masks shape after filtering: {self.masks.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.images) == 0 or len(self.masks) == 0:\n",
    "            raise IndexError(\"Dataset is empty. Ensure filtering did not remove all data.\")\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        return image, mask\n",
    "\n",
    "images_dir = \"images path\"\n",
    "masks_dir = \"labels path\"\n",
    "dataset = SegmentationDataset(images_dir=images_dir, masks_dir=masks_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44032723-de76-4823-8a2f-1bb222a7ccd9",
   "metadata": {},
   "source": [
    "## Filtering images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f10499a-bd35-4f38-9caa-c3a76a28b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "dataset_dict = {\n",
    "    \"image\": [],\n",
    "    \"label\": [],\n",
    "}\n",
    "target_size = (256, 256)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image_np, mask_np = dataset[i]\n",
    "\n",
    "    image_pil = Image.fromarray(image_np)\n",
    "    mask_pil = Image.fromarray(mask_np)\n",
    "    image_resized = image_pil.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    mask_resizeed = mask_pil.resize(target_size, Image.Resampling.NEAREST)\n",
    "\n",
    "    dataset_dict[\"image\"].append(image_resized)\n",
    "    dataset_dict[\"label\"].append(mask_resized)\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0ccf3-c1b3-4e59-83d3-735f4febb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730aed6-54dd-480d-bcc5-0d5bb09d07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "img_num = random.randint(0, image_np.shape[0]-1)\n",
    "example_image = dataset[img_num][\"image\"]\n",
    "example_mask = dataset[img_num][\"label\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "axes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95ef62b5-4bc6-4731-9f08-a8688e8151f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c6acf02-2c49-4890-83b4-a70d72e7e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  \"\"\"\n",
    "  This class is used to create a dataset that serves input images and masks.\n",
    "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da079914-201d-45d1-bff2-ae48c840dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "301112f3-89f9-443b-8b51-989fd809f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566b555-e62d-4a44-86a4-76b7b343b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "45d13134-d147-4ef4-a241-f30ca4f37dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272bdbd-f24d-46b5-96a5-53037109acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bafce-b074-4067-8fbb-7a8e624d3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"ground_truth_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9e276-78f3-4b0c-b602-ecd0a649884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "955b7f61-3e4c-4517-9b81-c7d68eed150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ead5c-50d2-4de1-8b48-0f08bfb56dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df8ee7-a9d5-4bc2-a2db-34290d5bf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0484205-c4fe-4085-885b-6b91f3581c33",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.save(model.state_dict(), \"weight/checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "817d95cf-f7e1-48e3-bd2e-a6d0f9dfd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4cb3a669-172b-499a-b5e4-4f4b6f910796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "my_mito_model = SamModel(config=model_config)\n",
    "my_mito_model.load_state_dict(torch.load(\"weight/mito_model_checkpoint.pth\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8cac8-50b0-4cdf-baba-a091bbc5bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "my_mito_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f34ea-8c77-4afc-ab0e-2e45fdeab4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = random.randint(0, image_np.shape[0]-1)\n",
    "print(idx)\n",
    "\n",
    "test_image = dataset[idx][\"image\"]\n",
    "\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "my_mito_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = my_mito_model(**inputs, multimask_output=False)\n",
    "\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(np.array(test_image), cmap='gray') \n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "axes[1].imshow(medsam_seg, cmap='gray') \n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "axes[2].imshow(medsam_seg_prob)  \n",
    "axes[2].set_title(\"Probability Map\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "output_path = \"Sam_result.png\"\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327461c-da77-4505-90a0-e4c3c2885411",
   "metadata": {},
   "source": [
    "## Randomly picking prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1f642-e18c-4e4f-b617-1a26d51a6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = random.randint(0, len(dataset) - 1)\n",
    "print(f\"Selected index: {idx}\")\n",
    "\n",
    "test_image = dataset[idx][\"image\"]\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "\n",
    "def get_normalized_points(mask, num_points=50):\n",
    "    pos_points = np.argwhere(mask == 1)  \n",
    "    neg_points = np.argwhere(mask == 0)  \n",
    "    \n",
    "    if len(pos_points) > 0:\n",
    "        pos_indices = np.random.choice(len(pos_points), min(num_points, len(pos_points)), replace=False)\n",
    "        pos_points = pos_points[pos_indices]\n",
    "    else:\n",
    "        pos_points = np.array([[0, 0]]) \n",
    "\n",
    "    if len(neg_points) > 0:\n",
    "        neg_indices = np.random.choice(len(neg_points), min(num_points, len(neg_points)), replace=False)\n",
    "        neg_points = neg_points[neg_indices]\n",
    "    else:\n",
    "        neg_points = np.array([[0, 0]])  \n",
    "\n",
    "    height, width = mask.shape\n",
    "    pos_points = [[y / width, x / height] for x, y in pos_points]\n",
    "    neg_points = [[y / width, x / height] for x, y in neg_points]\n",
    "\n",
    "    return pos_points, neg_points\n",
    "\n",
    "pos_points, neg_points = get_normalized_points(ground_truth_mask)\n",
    "\n",
    "points = pos_points + neg_points\n",
    "labels = [1] * len(pos_points) + [0] * len(neg_points)\n",
    "\n",
    "inputs = processor(test_image, input_points=[points], input_labels=[labels], return_tensors=\"pt\")\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "my_mito_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = my_mito_model(**inputs, multimask_output=False)\n",
    "\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(np.array(test_image))\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "axes[1].imshow(ground_truth_mask, cmap='gray')\n",
    "axes[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "axes[2].imshow(medsam_seg, cmap='gray')\n",
    "axes[2].set_title(\"Predicted Mask\")\n",
    "\n",
    "axes[3].imshow(medsam_seg_prob, cmap='gray')\n",
    "axes[3].set_title(\"Probability Map\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "height, width = ground_truth_mask.shape\n",
    "for point in pos_points:\n",
    "    axes[0].scatter(point[0] * width, point[1] * height, c='red', s=50)  \n",
    "for point in neg_points:\n",
    "    axes[0].scatter(point[0] * width, point[1] * height, c='blue', s=50)  \n",
    "output_path = \"Sam_result.png\"\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
